{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMmGAmFhjMVCfrwDgDpfa0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paro2708/SER517_Group35_Capstone/blob/GazeRefineNet/Trial_EyeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KiWTYXJU7Cdx"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EyeNet(nn.Module):\n",
        "  def __init__(self, side='left', use_rnn=True):\n",
        "    super(EyeNet, self).__init__()\n",
        "    self.resnet = models.resnet18(\n",
        "        #block = models.resnet.BasicBlock,\n",
        "        #layers = [2,2,2,2],\n",
        "        pretrained=False,        # Not using pre-trained weights\n",
        "        num_classes=128,        # (IMP)Number of output classes - needs to be defined based on number of eye features needed\n",
        "        norm_layer=nn.InstanceNorm2d,  # Normalization layer - layers need to be added\n",
        "    )\n",
        "    self.use_rnn = use_rnn\n",
        "\n",
        "    # Optional recurrent component - GRUCell\n",
        "    if use_rnn:\n",
        "            self.rnn = nn.GRUCell(input_size=128, hidden_size=128)\n",
        "            #self.fc_gaze = nn.Linear(128, 3)  # Output size for gaze direction\n",
        "\n",
        "    self.fc_gaze = nn.Sequential(\n",
        "                    nn.Linear(128, 128),\n",
        "                    nn.SELU(inplace=True),\n",
        "                    nn.Linear(128, 2, bias=False),\n",
        "                    nn.Tanh(),\n",
        "                )  # Output size for gaze direction\n",
        "    self.fc_pupil = nn.Sequential(\n",
        "                      nn.Linear(128, 128),\n",
        "                      nn.SELU(inplace=True),\n",
        "                      nn.Linear(128, 1),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                  )  # Output size for pupil size\n",
        "\n",
        "  def forward(self, input_eye_image, rnn_output=None):\n",
        "\n",
        "    features = self.resnet(input_eye_image)\n",
        "\n",
        "    if self.use_rnn:\n",
        "      rnn_features=features\n",
        "      batch_size, feature_size = features.shape\n",
        "      hidden = torch.zeros(batch_size, 128, device=rnn_features.device)\n",
        "      previous_results = []\n",
        "      output=[]\n",
        "\n",
        "      for i in range(batch_size):  # Loop through layers\n",
        "\n",
        "        if rnn_output is not None:\n",
        "            previous_results = output[i-1] if i>0 else None\n",
        "\n",
        "        GRUResult= self.rnn(rnn_features,hidden)\n",
        "\n",
        "        if isinstance(GRUResult, tuple):\n",
        "          rnn_features=GRUResult[0]\n",
        "          output[i] = GRUResult\n",
        "        else:\n",
        "          rnn_features = GRUResult\n",
        "          output.append(GRUResult)\n",
        "\n",
        "      features=rnn_features\n",
        "\n",
        "    #to calculate point of gaze\n",
        "    gaze_direction = (0.5 * np.pi) * self.fc_gaze(features)\n",
        "    gaze_direction_vector= convert_angles_to_vector(gaze_direction)\n",
        "    #x1,y1,x2,y2 need to be taken from meta data files for each eye - side to be given as input to eyenet\n",
        "    print(gaze_direction_vector[0][2])\n",
        "    origin = calculate_gaze_origin_direction(torch.tensor([0. ,0. ,gaze_direction_vector[0][2]]), z1=0, z2=0)\n",
        "    point_of_gaze_mm = calculate_intersection_with_screen(origin,gaze_direction_vector)\n",
        "    #Hard coding device dimenions from meta data\n",
        "    #screen_pixels from meta\n",
        "    screen_size_mm = [123.8 , 53.7]\n",
        "    screen_size_pixels = [568 , 320]\n",
        "    point_of_gaze_px = mm_to_pixels(point_of_gaze_mm,screen_size_mm, screen_size_pixels) # need to get from screen.json\n",
        "    pupil_size =self.fc_pupil(features)\n",
        "    print(\"Gaze Direction shape before linear layer:\", gaze_direction.shape)\n",
        "    print(\"Pupil Size shape before linear layer:\", pupil_size.shape)\n",
        "    return gaze_direction, pupil_size , point_of_gaze_px\n",
        "\n",
        "#Converting pitch and yaw to a vector - to convert gaze direction to a vector\n",
        "\n",
        "def convert_angles_to_vector(angles):\n",
        "    # Check if the input angles are 2-dimensional (pitch and yaw)\n",
        "    if angles.shape[1] == 2:\n",
        "        sine_values = torch.sin(angles)\n",
        "        cosine_values = torch.cos(angles)\n",
        "        # Construct and return the direction vector\n",
        "        return torch.stack([cosine_values[:, 0] * sine_values[:, 1], sine_values[:, 0], cosine_values[:, 0] * cosine_values[:, 1]], dim=1)\n",
        "    # Normalize the vector if the input is 3-dimensional\n",
        "    elif angles.shape[1] == 3:\n",
        "        return F.normalize(angles, dim=1)\n",
        "    else:\n",
        "        # Raise an error for unsupported input dimensions\n",
        "        raise ValueError(f'Unexpected input dimensions: {angles.shape}')\n",
        "\n",
        "def apply_transformation(T, vec):\n",
        "    if vec.shape[1] == 2:\n",
        "        vec = convert_angles_to_vector(vec)\n",
        "    vec = vec.reshape(-1, 3, 1)\n",
        "    h_vec = F.pad(vec, pad=(0, 0, 0, 1), value=1.0)\n",
        "    if T.size(-2) != 4 or T.size(-1) != 4:\n",
        "        raise ValueError(\"Transformation matrix T must be of shape [4, 4]\")\n",
        "    return torch.matmul(T, h_vec)[:, :3, 0]\n",
        "\n",
        "\n",
        "def apply_rotation(T, vec):\n",
        "    if vec.shape[1] == 2:\n",
        "        vec = convert_angles_to_vector(vec)\n",
        "    vec = vec.reshape(-1, 3, 1)\n",
        "    if T.dim() == 2:\n",
        "        T = T.unsqueeze(0)  # Add a batch dimension if it's missing\n",
        "    elif T.dim() != 3:\n",
        "        raise ValueError(\"T must be a 2D or 3D tensor\")\n",
        "    R = T[:, :3, :3]\n",
        "    return torch.matmul(R, vec).reshape(-1, 3)\n",
        "\n",
        "# To calculate point of gaze, gaze origin assumed to be 0,0,0\n",
        "def calculate_intersection_with_screen(o, direction):\n",
        "\n",
        "    # Ensure o and direction are 2D tensors [N, 3]\n",
        "    if o.dim() == 1:\n",
        "        o = o.unsqueeze(0)  # Add batch dimension if necessary\n",
        "    if direction.dim() == 1:\n",
        "        direction = direction.unsqueeze(0)  # Add batch dimension if necessary\n",
        "\n",
        "    rotation = torch.tensor([\n",
        "    [0.99970895052,-0.017290327698, 0.0168244000524],\n",
        "    [-0.0110340490937,0.292467236519, 0.9562118053443],\n",
        "    [-0.0214538034052,-0.956119179726,0.292191326618]\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    # Assuming no translation, and the camera is at the origin of the world space\n",
        "    camera_transformation_matrix = torch.eye(4)\n",
        "    camera_transformation_matrix[:3, :3] = rotation\n",
        "    inverse_camera_transformation_matrix = torch.inverse(camera_transformation_matrix)\n",
        "\n",
        "    # De-rotate gaze vector\n",
        "    inv_rotation = torch.inverse(rotation)\n",
        "    direction = direction.reshape(-1, 3, 1)\n",
        "    direction = torch.matmul(inv_rotation, direction)\n",
        "\n",
        "    direction = apply_rotation(inverse_camera_transformation_matrix, direction)\n",
        "    o = apply_transformation(inverse_camera_transformation_matrix, o)\n",
        "\n",
        "    # Assuming o = (0, 0, 0) for simplicity\n",
        "    # Solve for t when z = 0\n",
        "    t = -o[:, 2] / direction[:, 2]\n",
        "\n",
        "    # Calculate intersection point in millimeters\n",
        "    p_x = o[:, 0] + t * direction[:, 0]\n",
        "    p_y = o[:, 1] + t * direction[:, 1]\n",
        "\n",
        "    return torch.stack([p_x, p_y], dim=-1)\n",
        "\n",
        "def mm_to_pixels(intersection_mm, screen_size_mm, screen_size_pixels):\n",
        "    # Unpack screen dimensions\n",
        "    screen_height_mm, screen_width_mm = screen_size_mm\n",
        "    screen_height_px, screen_width_px = screen_size_pixels\n",
        "\n",
        "    # Calculate pixels per millimeter\n",
        "    ppmm_x = screen_width_px #/ screen_width_mm\n",
        "    ppmm_y = screen_height_px #/ screen_height_mm\n",
        "\n",
        "    # Convert intersection point from mm to pixels\n",
        "    intersection_px = intersection_mm * torch.tensor([ppmm_x, ppmm_y])\n",
        "    return intersection_px\n",
        "\n",
        "def calculate_gaze_origin_direction(z_gd, z1=0, z2=0):\n",
        "    # Convert points to tensors\n",
        "    x1 = 293\n",
        "    x2 = 346\n",
        "    y1 = 406\n",
        "    y2 = 405\n",
        "    point1 = torch.tensor([x1, y1, z1], dtype=torch.float32)\n",
        "    point2 = torch.tensor([x2, y2, z2], dtype=torch.float32)\n",
        "\n",
        "    # Calculate the vector pointing from point1 to point2\n",
        "    direction_vector = point2 - point1\n",
        "\n",
        "    # Normalize the vector to get a unit vector\n",
        "    unit_vector = direction_vector / torch.norm(direction_vector)\n",
        "\n",
        "    unit_vector= unit_vector + z_gd\n",
        "\n",
        "    return unit_vector"
      ],
      "metadata": {
        "id": "9j9icQCP7NS2"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "\n",
        "eyenet= EyeNet()\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = '/content/sample_data/0.jpg'\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "img = Image.open(image_path)\n",
        "img_tensor = preprocess(img)  # Add batch dimension\n",
        "\n",
        "# # Define a transform to convert the torch tensor to PIL image\n",
        "# transform = transforms.ToPILImage()\n",
        "\n",
        "# # Apply the transform to the torch tensor\n",
        "# image = transform(img_tensor)\n",
        "\n",
        "# # Save the PIL image to a file\n",
        "# image.save(\"/content/sample_data/image.png\")\n",
        "\n",
        "\n",
        "# print(img_tensor.unsqueeze(0).shape)\n",
        "\n",
        "# #%debug\n",
        "# # Assuming the EyeNet model is already defined and initialized as eyenet\n",
        "gaze_direction, pupil_size, point_of_gaze_px = eyenet(img_tensor.unsqueeze(0))\n",
        "\n",
        "print(\"Predicted Gaze Direction:\", gaze_direction)\n",
        "print(\"Predicted Pupil Size:\", pupil_size)\n",
        "print(\"Predicted Point of Gaze:\", point_of_gaze_px)\n",
        "magnitude = torch.norm(gaze_direction, p=2)\n",
        "magnitude = magnitude *(180/np.pi)\n",
        "print(\"Normalized Gaze Direction Magnitude(in radians):\", magnitude.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzV1HrG67mec",
        "outputId": "2926b5f9-ef4c-4f64-b7b8-a9cd54f12134"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9792, grad_fn=<SelectBackward0>)\n",
            "Gaze Direction shape before linear layer: torch.Size([1, 2])\n",
            "Pupil Size shape before linear layer: torch.Size([1, 1])\n",
            "Predicted Gaze Direction: tensor([[-0.1964, -0.0566]], grad_fn=<MulBackward0>)\n",
            "Predicted Pupil Size: tensor([[0.0453]], grad_fn=<ReluBackward0>)\n",
            "Predicted Point of Gaze: tensor([[ 306.7181, -612.0364]], grad_fn=<MulBackward0>)\n",
            "Normalized Gaze Direction Magnitude(in radians): 11.711485862731934\n"
          ]
        }
      ]
    }
  ]
}